import spacy
from spacy.training import Example
from spacy.util import minibatch, compounding
import random
import json
from pathlib import Path

def load_training_data(file_path):
    """Load training data from JSON file"""
    print(f"ðŸ“¥ Loading training data from {file_path}...")
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    print(f"âœ… Loaded {len(data)} training examples")
    return data

def convert_to_spacy_format(training_data):
    """Convert our format to spaCy's format"""
    spacy_data = []
    for item in training_data:
        entities = []
        for start, end, label in item["entities"]:
            entities.append((start, end, label))
        spacy_data.append((item["text"], {"entities": entities}))
    return spacy_data

def train_ner_model(training_data, output_dir="./custom_ner_model", n_iter=30, sample_size=None):
    """Train custom NER model
    
    Args:
        training_data: List of training examples
        output_dir: Where to save the model
        n_iter: Number of training iterations
        sample_size: If set, use only this many examples (for faster training)
    """
    
    print("\n" + "=" * 70)
    print("ðŸš€ STARTING TRAINING PROCESS")
    print("=" * 70)
    
    # Sample data if requested (useful for quick tests)
    if sample_size and sample_size < len(training_data):
        print(f"\nâš ï¸  Using {sample_size} examples (out of {len(training_data)}) for faster training")
        training_data = random.sample(training_data, sample_size)
    
    # Create a blank English model
    print("\nðŸ“ Creating blank spaCy model...")
    nlp = spacy.blank("en")
    
    # Create NER component
    if "ner" not in nlp.pipe_names:
        ner = nlp.add_pipe("ner")
    else:
        ner = nlp.get_pipe("ner")
    
    # Collect all unique labels
    print("\nðŸ·ï¸  Analyzing entity labels...")
    labels = set()
    for item in training_data:
        for start, end, label in item["entities"]:
            labels.add(label)
            ner.add_label(label)
    
    print(f"   Found {len(labels)} entity types: {sorted(labels)}")
    
    # Convert to spaCy format
    print("\nðŸ”„ Converting data to spaCy format...")
    train_data = convert_to_spacy_format(training_data)
    
    # Training
    print(f"\nðŸ“š Training for {n_iter} iterations...")
    print(f"   Dataset size: {len(train_data)} examples")
    print(f"   This may take 10-30 minutes depending on your computer...\n")
    
    # Initialize training
    nlp.begin_training()
    
    # Training loop
    for iteration in range(n_iter):
        random.shuffle(train_data)
        losses = {}
        
        # Create batches
        batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))
        
        for batch in batches:
            examples = []
            for text, annotations in batch:
                doc = nlp.make_doc(text)
                example = Example.from_dict(doc, annotations)
                examples.append(example)
            
            # Update model
            nlp.update(examples, drop=0.5, losses=losses)
        
        # Progress report
        if (iteration + 1) % 5 == 0 or iteration == 0:
            print(f"   âœ“ Iteration {iteration + 1:2d}/{n_iter} - Loss: {losses.get('ner', 0):8.2f}")
    
    # Save model
    print(f"\nðŸ’¾ Saving model to {output_dir}...")
    output_path = Path(output_dir)
    if not output_path.exists():
        output_path.mkdir(parents=True)
    
    nlp.to_disk(output_path)
    print("   âœ… Model saved successfully!")
    
    # Test the model
    print("\nðŸ§ª TESTING THE TRAINED MODEL")
    print("=" * 70)
    
    test_texts = [
        "Nishtha Sharma works at Microsoft in Pune, India.",
        "Elon Musk founded SpaceX in California in 2002.",
        "Apple CEO Tim Cook announced new products in Cupertino.",
        "The European Union imposed sanctions on Russia.",
        "Amazon acquired Whole Foods for $13.7 billion."
    ]
    
    for text in test_texts:
        doc = nlp(text)
        print(f"\nðŸ“ Text: {text}")
        if doc.ents:
            print("   Entities found:")
            for ent in doc.ents:
                print(f"      â€¢ {ent.text:20s} â†’ {ent.label_}")
        else:
            print("   âš ï¸  No entities detected")
    
    print("\n" + "=" * 70)
    print("ðŸŽ‰ TRAINING COMPLETE!")
    print("=" * 70)
    print(f"\nYour model is ready at: {output_dir}")
    print("\nNext steps:")
    print("1. Update main.py to load this model")
    print("2. Restart your backend server")
    print("3. Test in your web app!")
    print("=" * 70)

if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("           CUSTOM NER MODEL TRAINING")
    print("=" * 70)
    
    # Check which datasets are available
    conll_exists = Path("conll2003_training_data.json").exists()
    wnut_exists = Path("wnut17_training_data.json").exists()
    ontonotes_exists = Path("ontonotes_training_data.json").exists()
    
    if not conll_exists and not wnut_exists and not ontonotes_exists:
        print("\nâŒ ERROR: No training data found!")
        print("   Please run download_dataset.py first to download CoNLL-2003 or WNUT-17")
        exit(1)
    
    print("\nAvailable datasets:")
    if conll_exists:
        print("  1. CoNLL-2003 (conll2003_training_data.json)")
    if wnut_exists:
        print("  2. WNUT-17 (wnut17_training_data.json)")
    if ontonotes_exists:
        print("  3. OntoNotes 5.0 (ontonotes_training_data.json)")
    
    # Choose dataset
    if conll_exists and wnut_exists and ontonotes_exists:
        choice = input("\nWhich dataset to use? (1/2/3): ").strip()
        if choice == "1":
            data_file = "conll2003_training_data.json"
        elif choice == "2":
            data_file = "wnut17_training_data.json"
        elif choice == "3":
            data_file = "ontonotes_training_data.json"
    elif conll_exists:
        data_file = "conll2003_training_data.json"
        print(f"\n   Using: {data_file}")
    elif wnut_exists:
        data_file = "wnut17_training_data.json"
        print(f"\n   Using: {data_file}")
    else :
        data_file = "ontonotes_training_data.json"
        print(f"\n   Using: {data_file}")
    
    # Load data
    training_data = load_training_data(data_file)
    
    # Ask about sample size
    print(f"\nFull dataset has {len(training_data)} examples.")
    print("Training options:")
    print("  1. Quick training (1000 examples, ~5 minutes) - Good for testing")
    print("  2. Medium training (5000 examples, ~15 minutes) - Balanced")
    print("  3. Full training (all examples, ~30 minutes) - Best accuracy")
    
    train_choice = input("\nChoose option (1/2/3): ").strip()
    
    if train_choice == "1":
        sample_size = min(1000, len(training_data))
        n_iter = 20
    elif train_choice == "2":
        sample_size = min(5000, len(training_data))
        n_iter = 25
    else:
        sample_size = None
        n_iter = 30
    
    # Train model
    train_ner_model(training_data, sample_size=sample_size, n_iter=n_iter)